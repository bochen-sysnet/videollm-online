# Frame Difference Features Implementation

## âœ… Successfully Re-implemented

I've re-implemented the frame difference feature calculation on the clean version of `streaming_evaluate_event_driven.py`.

## ðŸŽ¯ Features Implemented

### Low-Level Image Difference Features (4 types)
For each frame compared to its previous frame within the same conversation:
1. **Corner Difference** - Harris corner detector response changes
2. **Edge Difference** - Canny edge detection changes
3. **Area Difference** - Contour area changes
4. **Pixel Difference** - Mean absolute pixel intensity changes

### Per-Conversation Tracking
- âœ… Features tracked separately for each conversation/stream
- âœ… Previous frames stored per conversation to avoid cross-contamination
- âœ… First frame of each conversation returns `None` for all features
- âœ… All conversations aggregated for unified visualization

## ðŸ“ Code Changes

### 1. Imports Added (lines 27-29)
```python
from scipy.stats import spearmanr
from collections import defaultdict
import cv2
```

### 2. New Functions (lines 114-253)
- `calculate_frame_diff_features(current_frame, prev_frame)` - Computes 4 image features
- `create_frame_features_vs_response_length_visualization(frame_features_data, ...)` - Creates 2x2 correlation plots

### 3. SimpleLiveInfer Class Updates

**New Instance Variables (lines 3153-3156)**:
```python
self.frame_features_data = {}  # Dict: conversation_id -> features list
self.prev_frame_per_conversation = {}  # Dict: conversation_id -> previous frame
self.current_conversation_id = None  # Active conversation tracker
```

**New Method (lines 3252-3259)**:
```python
def set_conversation_context(self, conversation_id):
    """Set the current conversation context for feature tracking."""
```

**Updated `input_video_stream()` (lines 3290-3309)**:
- Calculates frame features for each processed frame
- Stores features in conversation-specific list
- Updates previous frame for that conversation

**New Getter Methods (lines 3705-3720)**:
```python
def get_frame_features_data(self):
    """Get frame difference features data from all conversations."""
    
def update_frame_response_length(self, frame_idx, response_length, conversation_id):
    """Update the response length for a specific frame in a specific conversation."""
```

### 4. EventDrivenConversationContext Updates

**Updated `ensure_liveinfer_loaded()` (lines 1928, 1935)**:
- Calls `liveinfer.set_conversation_context(self.conversation_id)` on reset and restore

**Updated `handle_frame()` (lines 2029-2034)**:
- Calculates response length in words
- Updates frame features with response length

### 5. Main Evaluation Function (lines 2969-2973)
- Collects all frame features from all conversations
- Calls visualization function

## ðŸ“Š Output Visualization

**File**: `timing_plots/frame_features_vs_response_length_{data_source}.png`

**Content**:
- 2Ã—2 subplot grid (Corner, Edge, Area, Pixel differences)
- Scatter plots of feature values vs response lengths
- Trend lines showing correlations
- Pearson correlation coefficient (r)
- Spearman correlation coefficient (Ï) with p-values

**Console Output**:
```
ðŸ“Š Creating frame features vs response length visualization with 1234 frames...
âœ… Saved frame features visualization to timing_plots/frame_features_vs_response_length_goalstep.png

ðŸ“Š Correlation Summary:
  Corner Difference   : Pearson r= 0.234, Spearman Ï= 0.245 (p=1.234e-05)
  Edge Difference     : Pearson r= 0.156, Spearman Ï= 0.167 (p=2.345e-03)
  Area Difference     : Pearson r= 0.089, Spearman Ï= 0.091 (p=8.765e-02)
  Pixel Difference    : Pearson r= 0.312, Spearman Ï= 0.298 (p=3.456e-08)
```

## ðŸ—ï¸ Architecture

### Data Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conversation A  â”‚     â”‚ Conversation B  â”‚
â”‚ (video_1.mp4)   â”‚     â”‚ (video_2.mp4)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”œâ”€ set_context('A')     â”œâ”€ set_context('B')
         â”œâ”€ Frame 0 (prev=None)  â”œâ”€ Frame 0 (prev=None)
         â”œâ”€ Frame 1 (diff vs 0)  â”œâ”€ Frame 1 (diff vs 0)
         â””â”€ Frame 2 (diff vs 1)  â””â”€ Frame 2 (diff vs 1)
                â”‚                       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Aggregate All  â”‚
                   â”‚    Features     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Visualize      â”‚
                   â”‚  Correlations   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Per-Conversation Isolation
- Each conversation has independent frame tracking
- Previous frames stored separately per conversation
- No cross-contamination between different video streams
- All data aggregated only for final visualization

## ðŸš€ Usage

Simply run the evaluation:
```bash
python streaming_evaluate_event_driven.py
```

The implementation is **fully automatic**:
1. âœ… Features calculated during video processing
2. âœ… Response lengths tracked when generated
3. âœ… Visualization created at end of evaluation

## ðŸ“ˆ Interpreting Results

**Positive Correlation** (r > 0.3):
- Higher image change â†’ Longer responses
- VLM generates more detailed descriptions for dynamic scenes

**Negative Correlation** (r < -0.3):
- Higher image change â†’ Shorter responses
- VLM becomes more concise during dynamic scenes

**Low Correlation** (|r| < 0.3):
- Image features don't strongly influence response length
- Other factors (semantic content, prompts) dominate

**Statistical Significance**:
- p < 0.05: Correlation is statistically significant
- p â‰¥ 0.05: May be due to random chance

## ðŸ’¡ Key Benefits

1. **Per-Conversation Tracking**: No interference between concurrent streams
2. **Automatic**: No manual intervention required
3. **Comprehensive**: 4 different image feature types analyzed
4. **Statistical**: Both Pearson and Spearman correlations computed
5. **Visualized**: Clear 2x2 plot with trend lines

## ðŸ”§ Dependencies

All dependencies already installed:
- âœ… OpenCV 4.11.0
- âœ… PyTorch
- âœ… NumPy
- âœ… Matplotlib
- âœ… SciPy

## âœ¨ Summary

The frame difference features implementation is now **complete and ready to use** on your clean codebase! It correctly handles:
- Multiple concurrent conversations
- Per-conversation frame tracking
- Automatic response length calculation
- Unified correlation visualization
- Statistical significance testing

Run your evaluation and you'll get comprehensive insights into how image changes correlate with response verbosity! ðŸŽ‰

